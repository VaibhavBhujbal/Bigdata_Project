# Interview Assignment 

To demonstrate skillset of ETL, Hive and Spark. Please develop automation pipeline to ingest and calculate data in BigData Platform. 

1. Setup Hadoop Stack (Ex. Use Scripts like - Ansible, Bash etc to setup Hadoop Stack or Download Hortonworks Sandbox).
2. Develop scripts/programs to download and ingest data from <https://chronicdata.cdc.gov/views/735e-byxc/rows.csv?accessType=DOWNLOAD> to HDFS and Hive. 
3. Develop scripts/programs to calculate the following using Spark, and store the results into separate Hive tables:
	- Average of each Question’s "Data_Value" by year for all age groups
	- Average of each Question’s "Data_Value" by year for female only
4. Bonus (Optional) - Develop data visualization to show the data. (Ex. SpringBoot, React, JavaScript, HTML - D3.js)
5. Create a ***README.md*** file to include the execution steps to setup environment and run the solution.
6. Check-in the solution in GitHub.

